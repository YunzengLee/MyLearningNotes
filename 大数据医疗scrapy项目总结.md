scrapy框架分为五个部分：爬虫 引擎 调度器 下载器 管道 中间件

![](https://images2015.cnblogs.com/blog/931154/201703/931154-20170314141524729-978666187.png)

爬虫发起请求 通过引擎放到调度器里，引擎从调度器中取出请求交给下载器去与目标服务器交互，得到响应，传给引擎，引擎将响应交给爬虫去处理，管道再将爬虫所获取的信息进行保存（保存为json或进入数据库），中间件主要作用在爬虫与引擎之间 、和引擎与下载器之间。比较常用的是利用引擎和下载器之间的中间件，对将要向网络发起的请求进行处理，比如使用随机请求头或者使用代理ip来反爬虫。

- 需要编写的文件主要有spider item pipline

  1. 爬虫负责发起请求和解析响应，将解析得到的结果存放在item里。获得的响应可能是Http或者是json响应，前者可以用response.xpath()获取数据，后者用json.loads(response.text)得到字典对象。

     发起请求常用Request和FormRequest方法，里面主要参数有url、formdata、method（默认get）、headers、cookies、meta（将本层获得的数据传到下一层）、callback。
     Request返回的Response对象有以下属性： meta，上一层回调函数传来的额外数据；text，将传回来的数据作为Unicode字符串返回；body，作为bytes字符串返回；  xpath、css，都是选择器。

  2. items文件是scrapy提供的，可以自定义item类并自定义其中的属性，用于接收爬虫得到的数据，一般一个item类对应数据库中的一张表。

  3. 管道对item保存的数据进行处理，有init \start_spider\process_item\close_spider四个函数可以自定义，分别运行在初始化阶段，爬虫开始时，处理item时，爬虫关闭时。init和process_item常用。
     可以在init里连接数据库，提供主机名，端口，数据库名，用户及密码，在process_item里执行将item里的数据拼接为sql语句并执行。

- 数据库

  数据库中创建了9个表，其中一个患者表记录所有患者的基本信息（年龄，住址，电话），不包含检查结果，诊断结果，用药信息。用自增id作为主键。
  另外为每一种病症建立一个病历表，共8个表，比如肺炎 支气管炎。里面有一个外键设置为患者表的患者id，其余字段分别是姓名，检查结果，诊断信息，和用药。
  患者表和所有病历表都建立了自增id作为主键，因为其他字段基本都是字符串，可能有重复，而且用来做索引内存大且速度慢（字符串前缀索引的问题）。
  数据库在这个项目中主要是用于存储和查询，为了优化查询速度除了主键索引还建立了一些索引。在名字上建立了索引，因为名字的选择性比较高，而且字符串长度相对短。

- 反爬虫？？在下载器中间件上设置随机请求头和代理ip
  下载器中间件可以处理从引擎传递给下载器的请求， 以及下载器返回引擎的响应

利用中间件设置随机请求头和代理IP：
自定义一个中间件，添加到settings中，在中间件中定义两个函数process_request和process_response
在profess_request中：
随机请求头：定义一个随机请求头UserAgent的列表，随机选择一个，代替request.headers['User-Agent']里面的内容。

随机代理IP：定义一个代理IP列表，随机选择一个，替换META中的一个信息（忘了是啥了）

ajax
异步js和XML  xml是一种语法
数据交互一般使用json